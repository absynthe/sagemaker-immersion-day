{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automate deployments with Infrastructure-as-Code and CI/CD pipelines\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Creating a pipeline to build our Docker container image](#Creating-a-pipeline-to-build-our-Docker-container-image)\n",
    "1. [Adding the Step Functions to the infrastructure pipeline](#Adding-the-Step-Functions-to-the-infrastructure-pipeline)\n",
    "1. [Creating the ML Training and Deployment Pipeline](#Creating-the-ML-Training-and-Deployment-Pipeline)\n",
    "1. [Parametrizing our ML Workflow further](#Parametrizing-our-ML-Workflow-further)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this notebook, we will use the content developed in lab 3 and lab 4, to create the model training and model hosting docker image, as well as the step function based ML workflows and use CodePipeline pipelines to ensure that ML models are consistently and repeatedly deployed using a pipeline that defines how models move from development to production. Human quality gates can be included in the pipeline to have humans evaluate if a model is ready to deploy to a target environment.\n",
    "\n",
    "There will be an infrastructure-related pipeline, and a training and model management pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "### Creating a pipeline to build our Docker container image\n",
    "\n",
    "1. Download the Cloudformation template under `mlops/infra_pipeline.yml`\n",
    "2. Open the Amazon [Cloudformation console](https://console.aws.amazon.com/cloudformation/). \n",
    "3. Select **Create stack** and choose **With new resources (standard)**\n",
    "4. Choose **Template is ready** and **Upload a template file**. Search for the downloaded template above.\n",
    "5. Click **Next** and choose a stack-name, e.g. `sagemaker-immersionday-mlops-infra`\n",
    "6. Note the two parameters for the path of the **Buildspec** file used by CodeBuild, and ModelName.\n",
    "\n",
    "Run the cell below to check the contents of our Cloudformation template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat mlops/infra_pipeline.yml | pygmentize -l yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note above that the pipeline consists of a 'Source' stage and a 'Build' stage, as well as an S3 bucket to hold the pipeline's artifacts as they are pushed through its different stages. We are using the same role created in our main workshop stack `MLOpsRole` and the `Source` stage is the CodeCommit repo attached to this notebook, `master` branch. The actual build commands performed by the CodeBuild **build project** are described in our buildspec file under `mlops/docker_buildspec.yml`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat mlops/docker_buildspec.yml | pygmentize -l yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking docker build results\n",
    "\n",
    "After creating the stack above, the pipeline will run once with the contents of the `master` branch of our CodeCommit repo. The docker image will be built, tagged according to the version defined in the `container/version.txt` file and pushed to the same ECR repository created in Lab3 - **sagemaker-decision-trees**.\n",
    "\n",
    "Every time a new commit is made to the `master` branch of this repo, the pipeline will run and a new docker image published to the ECR repo.\n",
    "\n",
    "Change the version of the docker image, and push to CodeCommit - a new image in ECR with the new version tag should appear after the pipeline runs - [ECR console](https://console.aws.amazon.com/ecr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!printf \"VERSION=0.2.0\\nNAME=sagemaker-decision-trees\" > container/version.txt\n",
    "!git add container/version.txt\n",
    "!git commit -m \"Change version of container image\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the Step Functions to the infrastructure pipeline\n",
    "\n",
    "The state machines created in our previous notebook should also be versioned and deployed as IAC. For that we can use a function to output the Cloudformation template using the Step Functions SDK, that describes the state machine, as we did in the final step on the previous lab.\n",
    "\n",
    "Check the Step Functions YAML Cloudformation template:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cat mlops/train_workflow.yml | pygmentize -l yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's add the template to our Git repository, to be used in our updated infrastructure pipeline. But first edit the template to change the name of the state machine, not to conflict with the one created previously.\n",
    "\n",
    "After you've changed the `StateMachineName` property in the template `mlops/train_workflow.yml` above, commit it to our CodeCommit repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add mlops/train_workflow.yml\n",
    "!git commit -m \"Add first version of training state machine\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the various steps in the state machine under `DefinitionString`. Let's update our infrastructure pipeline that builds our container image, to include a step that deploys this Cloudformation template. Note that this could also be part of a separate pipeline, and the best workflow for building and deploying changes to the infrastructure will depend from case to case.\n",
    "\n",
    "1. Download the Cloudformation template under `mlops/infra_pipeline_stepfunctions.yml`\n",
    "2. Open the Amazon [Cloudformation console](https://console.aws.amazon.com/cloudformation/). \n",
    "3. Select the stack previously created and click on **Update**.\n",
    "4. Choose **Replace current template**. Search for the downloaded template above.\n",
    "5. Click **Next** and keep the same stack name and parameters defined before.\n",
    "6. Click **Next** again and finally **Update**\n",
    "\n",
    "After the Stack finishes updating, you can check the changed pipeline in the CodePipeline console. After the pipeline finishes running, an extra stack should be visible in the Cloudformation console with an extra **train_workflow** appended to the first pipeline stack name."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the ML Training and Deployment Pipeline\n",
    "\n",
    "CodePipeline has a native integration with Step Functions. Furthermore, you can use the codepipeline zipped artifact in S3 to provide input parameters to the pipeline stages.\n",
    "\n",
    "We will now create a separate pipeline that triggers on the upload to a specific S3 location of a set of parameters for a training job. This pipeline will execute the Step Functions state machine deployed above.\n",
    "\n",
    "Fill in the variables below, to pass to the training pipeline Cloudformation template (we will use the SageMaker default bucket for the location of the trigger). The TrainingStepFunctionName is the `StateMachineName` defined above in our Step Functions template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker # Amazon SageMaker's Python SDK provides many helper functions\n",
    "import boto3\n",
    "\n",
    "session = sagemaker.Session()\n",
    "region = boto3.Session().region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "S3Bucket=bucket\n",
    "S3Key=\"mlops/config.zip\"\n",
    "TrainingStepFunctionName=\"<Name of state machine created in Step Functions by our infra pipeline>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now launch the stack creation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws cloudformation deploy --template-file ./mlops/train_pipeline.yml --stack-name sm-id-train-pipeline --parameter-overrides S3DataBucket=$S3Bucket S3DataKey=$S3Key TrainingStepFunctionName=$TrainingStepFunctionName"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the new pipeline in the [Codepipeline](https://console.aws.amazon.com/codepipeline) and it will have failed in **Source** action, since we haven't created the trigger .zip file yet. Let's do that now. From the Step Functions state machine we previously created, we know that we can pass three parameters at runtime:\n",
    "\n",
    "```\n",
    "'JobName': str, \n",
    "'ModelName': str,\n",
    "'EndpointName': str\n",
    "```\n",
    "\n",
    "These are the parameters that need to be included in the config.json file for the training to be launched successfuly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime, json\n",
    "\n",
    "timestamp = datetime.datetime.now(tz=None).strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "params = {\n",
    "    'JobName': \"mlops-job-{}\".format(timestamp),\n",
    "    'ModelName': \"mlops-model-{}\".format(timestamp),\n",
    "    'EndpointName': \"mlops-endpoint-{}\".format(timestamp)\n",
    "}\n",
    "\n",
    "with open('config.json', 'w') as fp:\n",
    "    json.dump(params, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's zip the config file and upload it to the trigger location:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!zip config.zip config.json\n",
    "\n",
    "!aws s3 cp config.zip s3://$S3Bucket/$S3Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can follow the [CodePipeline](https://console.aws.amazon.com/codepipeline) execution and [State Machine](https://console.aws.amazon.com/states) execution in the consoles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parametrizing our ML Workflow further\n",
    "\n",
    "Currently we are allowing users to pass three variables at run-time, by specifying in a JSON-schema. We could do updates to our State Machine cloudformation template to allow more variables to be passed at run-time - e.g. instance type for training, container used for training, hyperparameters and so on.\n",
    "\n",
    "- Edit the file **mlops/train_workflow.yml**\n",
    "- Replace `ml.m4.4xlarge` with `$$.Execution.Input['InstanceType']`\n",
    "- Replace `813361260812.dkr.ecr.eu-central-1.amazonaws.com/xgboost:1` with `$$.Execution.Input['ContainerImage']`\n",
    "- Replace\n",
    "\n",
    "```json\n",
    "    \"HyperParameters\": {\n",
    "      \"max_depth\": \"5\",\n",
    "      \"eta\": \"0.2\",\n",
    "      \"gamma\": \"4\",\n",
    "      \"min_child_weight\": \"6\",\n",
    "      \"subsample\": \"0.8\",\n",
    "      \"silent\": \"0\",\n",
    "      \"objective\": \"binary:logistic\",\n",
    "      \"num_round\": \"100\"\n",
    "    }\n",
    "```\n",
    "\n",
    "    with:\n",
    "\n",
    "```json\n",
    "    \"HyperParameters\": {\n",
    "      \"max_depth.$\": \"$$.Execution.Input['max_depth']\",\n",
    "      \"eta.$\": \"$$.Execution.Input['eta']\",\n",
    "      \"gamma.$\": \"$$.Execution.Input['gamma']\",\n",
    "      \"min_child_weight.$\": \"$$.Execution.Input['min_child_weight']\",\n",
    "      \"subsample.$\": \"$$.Execution.Input['subsample']\",\n",
    "      \"silent.$\": \"$$.Execution.Input['silent']\",\n",
    "      \"objective.$\": \"$$.Execution.Input['objective']\",\n",
    "      \"num_round.$\": \"$$.Execution.Input['num_round']\"\n",
    "    }\n",
    "```\n",
    "\n",
    "Don't forget to add .$ to every variable in the template that will be resolved at runtime:\n",
    "\n",
    "e.g. `\"InstanceType.$\": \"$$.Execution.Input['InstanceType']\"`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, commit and push the modified state machine template. The Cloudformation stack in our infra pipeline will update with the new set of runtime parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git add mlops/train_workflow.yml\n",
    "!git commit -m \"Add more runtime inputs to training state machine\"\n",
    "!git push"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now trigger a new ML training and deployment workflow, using our custom Docker container image and a different instance type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.datetime.now(tz=None).strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "\n",
    "params = {\n",
    "    'JobName': \"mlops-job-{}\".format(timestamp),\n",
    "    'ModelName': \"mlops-model-{}\".format(timestamp),\n",
    "    'EndpointName': \"mlops-endpoint-{}\".format(timestamp),\n",
    "    'InstanceType': \"ml.m5.4xlarge\",\n",
    "    'ContainerImage': \"<account-id>.dkr.ecr.<region>.amazonaws.com/sagemaker-decision-trees:0.1.0\",\n",
    "    'HyperParameters': {\n",
    "      \"max_depth\": \"10\",\n",
    "      \"eta\": \"0.4\",\n",
    "      \"gamma\": \"4\",\n",
    "      \"min_child_weight\": \"5\",\n",
    "      \"subsample\": \"0.8\",\n",
    "      \"silent\": \"0\",\n",
    "      \"objective\": \"binary:logistic\",\n",
    "      \"num_round\": \"50\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open('config.json', 'w') as fp:\n",
    "    json.dump(params, fp)\n",
    "    \n",
    "!zip config.zip config.json\n",
    "\n",
    "!aws s3 cp config.zip s3://$S3Bucket/$S3Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CodePipeline is now running again - you can confirm that the training job is using the container defined above and our changed hyperparameters by navigating to the [SageMaker training jobs console](https://eu-central-1.console.aws.amazon.com/sagemaker/home?region=eu-central-1#/jobs)\n",
    "\n",
    "If you navigate to [SageMaker Models](https://eu-central-1.console.aws.amazon.com/sagemaker/home?region=eu-central-1#/models), you will find the trained model with the container used in training, as well as the location of the model artifact on S3. We could also track each of the training runs by using [SageMaker Experiments](https://docs.aws.amazon.com/sagemaker/latest/dg/experiments-create.html) when launching the training job. Notice that the new model uses our custom container.\n",
    "\n",
    "At the end of the run, an endpoint is created to serve inference requests in [Endpoints](https://eu-central-1.console.aws.amazon.com/sagemaker/home?region=eu-central-1#/endpoints).\n",
    "\n",
    "The endpoint creation could be extracted from the State Machine, and created as a Cloudformation template in a next stage of the pipeline - the same template could then be deployed to a Prod account after an approval stage in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
